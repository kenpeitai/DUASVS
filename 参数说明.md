### 训练参数说明（可复现实验口径）

### 方案A：无限耐心 + 原文 reward（线性 rebuf）+ 允许调系数（推荐起点）

- **适用场景**：你坚持 `max_rebuf_s_per_video=0`（无限耐心）时，原文线性 rebuf 惩罚（默认 `rebuf_penalty=2.66`）会被超大 `rebuf_s_per_step (~120s)` 线性放大，导致 `return_per_step` 长期被 rebuf 项“压死”（-300 左右），训练难以有效改善策略。  
  方案A 的目的就是：**先把 `rebuf_penalty` 降到可训的量级**，让策略先学会减少 rebuf；等 rebuf 明显下降后再逐步把 `rebuf_penalty` 调回原文量级做对照。

#### A-1. 推荐系数（先跑这个）
- **reward 形式**：保持原文（`--rebuf_penalty_mode linear`）
- **无限耐心**：`--max_rebuf_s_per_video 0`
- **QoE 系数（可调）**：
  - **rebuf_penalty（α）**：`0.2`
  - **switch_penalty（β）**：`1.0`
  - **r_min_mbps**：`0.2`
  - **lambda_waste（λ_waste）**：`1.0`

#### A-2. 训练命令片段（直接复制）

```bash
python train_duasvs_ppo.py \
  --mode official \
  --seed 0 \
  --device cuda \
  --total_steps 200000 \
  --events_csv data/big_matrix_valid_video.csv \
  --traces_csv data/fcc_httpgetmt_trace_1s_kbps.csv \
  --trace_ids_file splits/fcc_train_trace_ids.txt \
  --eval_ids_file splits/fcc_val_trace_ids.txt \
  --eval_n_traces 200 \
  --eval_select_mode shuffle \
  --eval_max_videos 1000 \
  --eval_every_updates 1 \
  --max_rebuf_s_per_video 0 \
  --rebuf_penalty_mode linear \
  --rebuf_penalty 0.2 \
  --switch_penalty 1.0 \
  --lambda_waste 1.0 \
  --r_min_mbps 0.2 \
  --run_name official_seed0_infinite_patience_linearreward_A
```

#### A-3. 如何判断是否需要继续调参（经验判据）
- **如果 `return_per_step` 仍长期在 -300 附近**：优先继续降低 `rebuf_penalty`（例如 0.2 → 0.1）。
- **如果 `return_per_step` 上升但 `waste_mbit_per_step` 明显增大**：提高 `lambda_waste`（例如 1.0 → 2.0）。

### 方案P：恢复耐心上限（推荐用于“无限耐心下 rebuf 降不下来”的情况）

- **核心变化**：启用用户耐心上限，避免单个视频出现几百秒的无界卡顿时间，从而让训练信号更稳定、更接近短视频用户行为。
- **耐心上限**：`--max_rebuf_s_per_video 8`（默认值）
- **reward 形式**：保持原文（线性 rebuf，`--rebuf_penalty_mode linear`）
- **QoE 系数（建议先用论文默认）**：
  - **rebuf_penalty（α）**：`2.66`
  - **switch_penalty（β）**：`1.0`
  - **r_min_mbps**：`0.2`
  - **lambda_waste（λ_waste）**：`1.0`

---

### Seed9 基线（当前对照组）：耐心10s + 方案A系数 + 关闭QoE观看比例缩放（NOwatchfrac）

- **用途**：作为“能冲到较高 best 但波动较大”的基线点，后续所有稳定化/收敛性改动都从这套参数出发对比。
- **关键开关**：
  - **耐心上限**：`--max_rebuf_s_per_video 10`
  - **方案A系数**：`--rebuf_penalty 4.0`，`--lambda_waste 0.5`
  - **关闭观看比例缩放**：`--no_qoe_scale_by_watch_frac`
  - **PPO**：`--lr 2e-4`，`--ent_coef 0.025`，`--clip_ratio 0.1`，`--train_iters 5`（batch_size 默认 256）
- **动作空间默认**（当前仓库默认）：
  - bitrates：`0.35,0.7,1.2,2.5,5.0`
  - prefetch_thresholds：`1,2,4,6,8,12,18,24,30,36`
- **评估口径**（official 模式默认）：
  - 每个 update 都 eval（`eval_every_updates=1`）
  - eval traces = 50 条（`eval_n_traces=50`，默认取 `splits/fcc_val_trace_ids.txt` 前 50 条）
  - 实际选中的 eval trace ids 已落盘：`checkpoints/official_seed9_patience10_schemeA_rebuf4_waste0p5_lr2e4_ent0p025_NOwatchfrac_eval_trace_ids_selected.txt`

#### Seed9 训练命令（原样保存，可直接复跑）

```bash
python train_duasvs_ppo.py \
  --mode official \
  --seed 9 \
  --device cuda \
  --total_steps 400000 \
  --max_rebuf_s_per_video 10 \
  --rebuf_penalty 4.0 \
  --lambda_waste 0.5 \
  --lr 2e-4 \
  --ent_coef 0.025 \
  --clip_ratio 0.1 \
  --train_iters 5 \
  --no_qoe_scale_by_watch_frac \
  --run_name official_seed9_patience10_schemeA_rebuf4_waste0p5_lr2e4_ent0p025_NOwatchfrac
```

---

### Seed74（当前实验配置）：soft-waste + 抗坍缩 PPO（从 seed73_best 微调）

#### Seed74 的 reward 构成（逐项）

令：
- \(r_{\min} = \max(\texttt{eps\_mbps}, \texttt{r\_min\_mbps})\)
- \(u(r) = \log(\max(\texttt{eps\_mbps}, r)/r_{\min})\)
- \(\texttt{watched\_frac} = \text{clip}\big(\frac{\texttt{played\_s}}{\max(1e-9,\texttt{watch\_s})}, 0, 1\big)\)

则每步（每视频决策一次）的 reward 为：
- **码率效用**：\(\texttt{qoe\_bitrate} = u(\texttt{bitrate\_mbps}) \cdot \texttt{watched\_frac}\)
- **切换惩罚（log 差）**：
  \[
  \texttt{qoe\_switch} = \left|u(\texttt{prev\_bitrate\_mbps}) - u(\texttt{bitrate\_mbps})\right|
  \]
- **卡顿惩罚（linear）**：\(\texttt{rebuf\_term} = \texttt{rebuf\_s}\)
- **soft-waste 惩罚（数据浪费比例 soft saturating）**：
  \[
  \texttt{raw} = \frac{\texttt{waste\_mbit}}{\max(\texttt{downloaded\_mbit}, 1e-9)},\quad
  \texttt{waste\_ratio} = 1 - \exp(-k\cdot \max(0,\texttt{raw}))
  \]
- **总 reward**：
  \[
  \texttt{reward} =
  \texttt{qoe\_bitrate}
  - \beta \cdot \texttt{qoe\_switch}
  - \alpha \cdot \texttt{rebuf\_term}
  - \lambda_{\text{waste}} \cdot \texttt{waste\_ratio}
  \]

#### Seed74 的 reward 系数（本次跑法）

- **rebuf_penalty（α）**：`1.0`（`--rebuf_penalty 1.0`，`--rebuf_penalty_mode linear`）
- **switch_penalty（β）**：`0.1`（`--switch_penalty 0.1`，`--switch_penalty_mode log`）
- **lambda_waste（λ_waste）**：`0.3`（`--lambda_waste 0.3`）
- **waste soft-saturating k**：`1.0`（`--waste_saturate_k 1.0`）
- **r_min_mbps**：`0.35`（`--r_min_mbps 0.35`）
- **qoe_alpha**：`1.0`（`--qoe_alpha 1`）

#### Seed74 的动作空间（本次跑法）

- **bitrates_mbps**：`0.35,0.6,0.9,1.2,1.8,2.5,4.0`
- **prefetch_thresholds（s）**：`1,3,6,10,16,30`

#### Seed74 的学习参数（PPO）

- **初始化**：从 `seed73_best.pt` 加载（`--init_ckpt checkpoints/fccmobile_seed73_finetune_from71best_lr5e5_tkl0p03_steps400k_best.pt`）
- **total_steps**：`400000`
- **rollout_steps**：`1024`
- **batch_size**：`256`
- **train_iters**：`4`
- **lr**：`1e-4`
- **clip_ratio**：`0.1`
- **target_kl**：`0.03`
- **ent_coef**：`0.02`（不退火：`--ent_anneal_steps 0`）
- **uniform_kl_coef**：`0.02`
- **vf_coef**：`0.5`
- **logits_clip**：`5`
- **rollout_action_mode**：`sample`
- **eval_every_updates**：`4`

#### Seed74 训练命令（原样保存，可复跑）

```bash
python train_duasvs_ppo.py \
  --mode official \
  --seed 74 \
  --device cuda \
  --total_steps 400000 \
  --rollout_steps 1024 \
  --batch_size 256 \
  --train_iters 4 \
  --lr 1e-4 \
  --clip_ratio 0.1 \
  --target_kl 0.03 \
  --ent_coef 0.02 \
  --ent_anneal_steps 0 \
  --uniform_kl_coef 0.02 \
  --vf_coef 0.5 \
  --logits_clip 5 \
  --rollout_action_mode sample \
  --eval_every_updates 4 \
  --max_rebuf_s_per_video 0 \
  --rebuf_penalty_mode linear \
  --rebuf_penalty 1.0 \
  --switch_penalty 0.1 \
  --switch_penalty_mode log \
  --lambda_waste 0.3 \
  --waste_saturate_k 1.0 \
  --r_min_mbps 0.35 \
  --qoe_alpha 1 \
  --bitrates_mbps \"0.35,0.6,0.9,1.2,1.8,2.5,4.0\" \
  --bitrate_labels \"180p,270p,360p,480p,720p,1080p,1440p\" \
  --prefetch_thresholds \"1,3,6,10,16,30\" \
  --trace_exhaust_mode drop_video \
  --eval_n_traces 50 \
  --eval_select_mode head \
  --init_ckpt checkpoints/fccmobile_seed73_finetune_from71best_lr5e5_tkl0p03_steps400k_best.pt \
  --run_name fccmobile_seed74_finetune_from73best_lr1e4_tkl0p03_steps400k
```
