### 训练参数说明（可复现实验口径）

### 方案A：无限耐心 + 原文 reward（线性 rebuf）+ 允许调系数（推荐起点）

- **适用场景**：你坚持 `max_rebuf_s_per_video=0`（无限耐心）时，原文线性 rebuf 惩罚（默认 `rebuf_penalty=2.66`）会被超大 `rebuf_s_per_step (~120s)` 线性放大，导致 `return_per_step` 长期被 rebuf 项“压死”（-300 左右），训练难以有效改善策略。  
  方案A 的目的就是：**先把 `rebuf_penalty` 降到可训的量级**，让策略先学会减少 rebuf；等 rebuf 明显下降后再逐步把 `rebuf_penalty` 调回原文量级做对照。

#### A-1. 推荐系数（先跑这个）
- **reward 形式**：保持原文（`--rebuf_penalty_mode linear`）
- **无限耐心**：`--max_rebuf_s_per_video 0`
- **QoE 系数（可调）**：
  - **rebuf_penalty（α）**：`0.2`
  - **switch_penalty（β）**：`1.0`
  - **r_min_mbps**：`0.2`
  - **lambda_waste（λ_waste）**：`1.0`

#### A-2. 训练命令片段（直接复制）

```bash
python train_duasvs_ppo.py \
  --mode official \
  --seed 0 \
  --device cuda \
  --total_steps 200000 \
  --events_csv data/big_matrix_valid_video.csv \
  --traces_csv data/fcc_httpgetmt_trace_1s_kbps.csv \
  --trace_ids_file splits/fcc_train_trace_ids.txt \
  --eval_ids_file splits/fcc_val_trace_ids.txt \
  --eval_n_traces 200 \
  --eval_select_mode shuffle \
  --eval_max_videos 1000 \
  --eval_every_updates 1 \
  --max_rebuf_s_per_video 0 \
  --rebuf_penalty_mode linear \
  --rebuf_penalty 0.2 \
  --switch_penalty 1.0 \
  --lambda_waste 1.0 \
  --r_min_mbps 0.2 \
  --run_name official_seed0_infinite_patience_linearreward_A
```

#### A-3. 如何判断是否需要继续调参（经验判据）
- **如果 `return_per_step` 仍长期在 -300 附近**：优先继续降低 `rebuf_penalty`（例如 0.2 → 0.1）。
- **如果 `return_per_step` 上升但 `waste_mbit_per_step` 明显增大**：提高 `lambda_waste`（例如 1.0 → 2.0）。

### 方案P：恢复耐心上限（推荐用于“无限耐心下 rebuf 降不下来”的情况）

- **核心变化**：启用用户耐心上限，避免单个视频出现几百秒的无界卡顿时间，从而让训练信号更稳定、更接近短视频用户行为。
- **耐心上限**：`--max_rebuf_s_per_video 8`（默认值）
- **reward 形式**：保持原文（线性 rebuf，`--rebuf_penalty_mode linear`）
- **QoE 系数（建议先用论文默认）**：
  - **rebuf_penalty（α）**：`2.66`
  - **switch_penalty（β）**：`1.0`
  - **r_min_mbps**：`0.2`
  - **lambda_waste（λ_waste）**：`1.0`

---

### Seed9 基线（当前对照组）：耐心10s + 方案A系数 + 关闭QoE观看比例缩放（NOwatchfrac）

- **用途**：作为“能冲到较高 best 但波动较大”的基线点，后续所有稳定化/收敛性改动都从这套参数出发对比。
- **关键开关**：
  - **耐心上限**：`--max_rebuf_s_per_video 10`
  - **方案A系数**：`--rebuf_penalty 4.0`，`--lambda_waste 0.5`
  - **关闭观看比例缩放**：`--no_qoe_scale_by_watch_frac`
  - **PPO**：`--lr 2e-4`，`--ent_coef 0.025`，`--clip_ratio 0.1`，`--train_iters 5`（batch_size 默认 256）
- **动作空间默认**（当前仓库默认）：
  - bitrates：`0.35,0.7,1.2,2.5,5.0`
  - prefetch_thresholds：`1,2,4,6,8,12,18,24,30,36`
- **评估口径**（official 模式默认）：
  - 每个 update 都 eval（`eval_every_updates=1`）
  - eval traces = 50 条（`eval_n_traces=50`，默认取 `splits/fcc_val_trace_ids.txt` 前 50 条）
  - 实际选中的 eval trace ids 已落盘：`checkpoints/official_seed9_patience10_schemeA_rebuf4_waste0p5_lr2e4_ent0p025_NOwatchfrac_eval_trace_ids_selected.txt`

#### Seed9 训练命令（原样保存，可直接复跑）

```bash
python train_duasvs_ppo.py \
  --mode official \
  --seed 9 \
  --device cuda \
  --total_steps 400000 \
  --max_rebuf_s_per_video 10 \
  --rebuf_penalty 4.0 \
  --lambda_waste 0.5 \
  --lr 2e-4 \
  --ent_coef 0.025 \
  --clip_ratio 0.1 \
  --train_iters 5 \
  --no_qoe_scale_by_watch_frac \
  --run_name official_seed9_patience10_schemeA_rebuf4_waste0p5_lr2e4_ent0p025_NOwatchfrac
```
